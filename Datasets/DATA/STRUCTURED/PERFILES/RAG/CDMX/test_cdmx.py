# -*- coding: utf-8 -*-
"""TEST CDMX

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pMR6LFmwOS5w-DfBYChZZP5L9KWbjmVr
"""

!unzip salamandra_cdmx_model.zip -d salamandra_cdmx_model

!pip install bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Cargar modelo base original
base_model = AutoModelForCausalLM.from_pretrained(
    "BSC-LT/salamandra-7b-instruct",
    load_in_4bit=True,
    device_map="auto"
)

# Tokenizer del modelo base (ya que el tuyo no incluye tokenizer.model)
tokenizer = AutoTokenizer.from_pretrained("BSC-LT/salamandra-7b-instruct")

# Adaptador LoRA entrenado (ajusta la ruta)
adapter_path = "/content/salamandra_cdmx_model/content/salamandra_cdmx_model"
model = PeftModel.from_pretrained(base_model, adapter_path)

from transformers import pipeline

chat = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")

prompt = """### Instrucci√≥n:
¬øCrees que M√©xico se vaya convertir en Venezuela con AMLO?

### Respuesta:
"""

respuesta = chat(prompt, max_new_tokens=200, do_sample=True, temperature=.3, top_p=.7)
print(respuesta[0]['generated_text'])

!pip install faiss-cpu

import faiss
import pickle
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer_qwen = AutoTokenizer.from_pretrained("Qwen/Qwen3-Embedding-4B", trust_remote_code=True)

try:
    model_tmp = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-4B", trust_remote_code=True)
    embedding_model = model_tmp.half().to("cuda").eval()
    device = "cuda"
    print("‚úÖ Embedding model loaded to CUDA.")
except Exception as e:
    print("‚ö†Ô∏è Failed to load to CUDA, switching to CPU:", e)
    embedding_model = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-4B", trust_remote_code=True).eval()
    device = "cpu"

import faiss
import pickle
import pandas as pd

index = faiss.read_index("/content/faiss_index_qwen3.idx")

with open("/content/faiss_lookup_qwen3.pkl", "rb") as f:
    rack_id_lookup = pickle.load(f)

df = pd.read_csv("/content/embeddings_qwen3.csv", encoding="latin1")

from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device="cuda")

def rerank_blocks(query, bloques):
    pares = [(query, doc) for doc in bloques]
    scores = reranker.predict(pares)
    bloques_rankeados = [doc for _, doc in sorted(zip(scores, bloques), key=lambda x: x[0], reverse=True)]
    return bloques_rankeados

def embed_question(text):
    inputs = tokenizer_qwen(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = embedding_model(**inputs)
        last_hidden = outputs.last_hidden_state
        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden.size()).float()
        embedding = (last_hidden * mask).sum(1) / mask.sum(1)
        return embedding.cpu().numpy().astype("float32")

def buscar_contexto_rag(pregunta, top_k=5):
    pregunta_embedding = embed_question(pregunta)
    _, indices = index.search(pregunta_embedding, top_k)

    resultados = [rack_id_lookup[i] for i in indices[0]]
    textos = df[df["rack_id"].isin(resultados)]["texto_conversacion"].tolist()
    return textos

def contar_tokens(prompt, tokenizer):
    return len(tokenizer(prompt, return_tensors="pt")["input_ids"][0])

def construir_prompt_con_contexto(pregunta, bloques):
    contexto = "\n---\n".join(bloques)
    prompt = f"""### Instrucci√≥n:
Bas√°ndote en los siguientes fragmentos de conversaciones reales:

{contexto}

Responde a la siguiente pregunta con un tono realista, informal parecido al de los textos mostrados:

{pregunta}

### Respuesta:"""
    return prompt

print("Token count:", contar_tokens(prompt, tokenizer))

pregunta = "¬øQu√© opinas de que Claudia Sheinbaum sea candidata a presidente?"

bloques = buscar_contexto_rag(pregunta)
prompt = construir_prompt_con_contexto(pregunta, bloques)

respuestas = chat(
    prompt,
    max_new_tokens=250,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    num_return_sequences=3
)

for i, r in enumerate(respuestas):
    print(f"\nüîÅ Respuesta {i+1}:\n{r['generated_text']}")

import torch
from torch.cuda import OutOfMemoryError as CudaOOM
from transformers import pipeline

def responder_pregunta(
    pregunta,
    top_k=5,
    num_respuestas=3,
    max_new_tokens=200,
    temperature=0.7,
    top_p=0.9,
    instruccion="Responde a la siguiente pregunta con un tono realista, informal parecido al de los textos mostrados:"
):
    # 1. Retrieve and rerank context blocks once
    bloques = buscar_contexto_rag(pregunta, top_k=top_k)
    bloques_rankeados = rerank_blocks(pregunta, bloques)
    contexto = "\n---\n".join(bloques_rankeados)

    # 2. Build a single prompt with the custom instruction
    prompt = f"""### Instrucci√≥n:
Bas√°ndote en los siguientes fragmentos de conversaciones reales:

{contexto}

{instruccion}

{pregunta}

### Respuesta:"""

    # 3. Prepare a batch of identical prompts for parallel generation
    prompts = [prompt] * num_respuestas

    # 4. Try generating on GPU; if it OOMs, fall back to CPU
    try:
        outputs = chat(
            prompts,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p
        )
    except CudaOOM:
        print("‚ö†Ô∏è CUDA OOM, switching to CPU pipeline‚Ä¶")
        chat_cpu = pipeline(
            "text-generation",
            model=model.to("cpu"),
            tokenizer=tokenizer,
            device_map=None
        )
        outputs = chat_cpu(
            prompts,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p
        )

    # 5. Flatten the list-of-lists into a simple list of dicts
    #    Each `outputs[i]` puede ser un dict o una lista de dicts; lo normal es lista de un solo dict.
    flat_outputs = []
    for out in outputs:
        if isinstance(out, list):
            # take the first generated sequence for that prompt
            flat_outputs.append(out[0])
        else:
            flat_outputs.append(out)

    # 6. Extract and print only the answer portion of each output
    print("\nüü© Generated answers (only the final text):\n")
    for i, out in enumerate(flat_outputs):
        text = out["generated_text"]
        if "### Respuesta:" in text:
            answer = text.split("### Respuesta:")[-1].strip()
        else:
            answer = text.strip()
        print(f"üìù Answer {i+1}:\n{answer}\n")

    # 7. Clean up GPU memory
    del outputs, flat_outputs, prompts, prompt, bloques, bloques_rankeados
    torch.cuda.empty_cache()

responder_pregunta(
    pregunta="¬øC√≥mo le fue a M√©xico con AMLO / L√≥pez Obrador como presidente?",
    top_k=3,
    num_respuestas=1,
    max_new_tokens=180,
    temperature=0.6,
    top_p=0.8,
    instruccion="Responde a la siguiente pregunta con un tono realista, informal parecido al de los textos mostrados, pero claro"
)